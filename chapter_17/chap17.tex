\documentclass[12pt]{report}

% Packages
\usepackage{acronym}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{epigraph}
\usepackage{geometry}
\usepackage{graphicx}
%\usepackage{mdframed}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{siunitx}
\usepackage{enumitem}
\usepackage{pdfpages}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{transparent}
\usepackage[hidelinks, colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue]{hyperref}
\usepackage[backend=biber, style=ieee]{biblatex}
%\addbibresource{cdt_app.bib}

% Acronym set-up
% Command to insert acronyms with a temporary color change
\newcommand{\myAcro}[1]{{\hypersetup{linkcolor=black}\acl{#1}}} % For acronym list (full form)
\newcommand{\myAcroLink}[1]{{\hypersetup{linkcolor=black}\acs{#1}}}

% Document geometry setup
\geometry{
    a4paper,
    margin=2cm,
    top=1cm,    % Adjust top margin
    bottom=1cm,  % Adjust bottom margin
    headheight=14.5pt,
    includeheadfoot
}

\pagestyle{fancy} % Use fancy style for all pages
\fancyhf{} % Clear all header and footer fields

% Header
\fancyhead[L]{{\transparent{0.5}Chapter \udlchap{} Questions}} % Left header
\fancyhead[R]{{\transparent{0.5}wp289}} % Right header

% Footer
\fancyfoot[L]{\transparent{0.5}Understanding Deep Learning} % Left footer
\fancyfoot[R]{\transparent{0.5}\thepage} % Right footer

% Define line thickness
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}

% Redefining plain page style if needed
\fancypagestyle{plain}{ % Applies to chapter beginnings and similar
  \fancyhf{} % clear all header and footer fields for plain style
  \renewcommand{\headrulewidth}{0.4pt}
  \renewcommand{\footrulewidth}{0.4pt}
  \fancyhead[L]{{\transparent{0.5}Chapter \udlchap{}Questions}}
  \fancyhead[R]{{\transparent{0.5}wp289}}
  \fancyfoot[L]{\transparent{0.5}Understanding Deep Learning}
  \fancyfoot[R]{\transparent{0.5}\thepage}
}

% Customise Chapter Headings
\titleformat{\chapter}
    {\normalfont\LARGE\bfseries}
    {\thechapter.}
    {1em}
    {}
\titlespacing*{\chapter}{0pt}{-20pt}{\baselineskip}

% Customise section spacing
\titlespacing*{\section}{0pt}{\baselineskip}{0.5\baselineskip}

% Adjusting space around figures
\setlength{\floatsep}{5pt}
\setlength{\textfloatsep}{5pt}

% Adjusting caption spacing
\captionsetup{aboveskip=5pt, belowskip=5pt}

% Format paragraphs
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}  % Adjust the value as needed

% Adjust list spacing
%\setlist[enumerate]{before=\vspace{0.5em}, after=\vspace{0.5em}}
\setlist[enumerate]{before=\vspace{-0.5\baselineskip}, after=\vspace{-0.5\baselineskip}}

% Customise question boxes
\usepackage[framemethod=TikZ]{mdframed}
\mdfsetup{
    backgroundcolor=gray!20,
    innertopmargin=10pt,
    innerbottommargin=10pt,
    skipbelow=0pt 
}

\def\udlchap{17}
\renewcommand{\thesubsection}{Problem \udlchap.\arabic{subsection}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\subsection{}
\begin{mdframed}
    How many parameters are needed to create a 1D mixture of Gaussians with $n=5$ components (equation 17.4)? State the possible range of values that each parameter could take.

    \begin{equation*}
        P(x) = \sum_{i=1}^{n} \lambda_{n} \cdot \text{Norm}_{x}[\mu_{n}, \sigma_{n}^{2}]
        \label{eq:17.4}
        \tag{17.4}
    \end{equation*}
\end{mdframed}

In a 1D mixture of Gaussians, the latent variable $z$ is discrete, and the prior $P(z)$ is a categorical distribution with one probability $\lambda_n$ for each possible valye of $z$. The likelihood $P(x | z =n)$ of the data $x$ given that the latent variable takes value $n$ is normally distributed with mean $\mu_n$ and variance $\sigma_n^2$. The marginal distribution of the data $P(x)$ is obtained by summing over all possible values of the latent variable.

Therefore, since the mixing coefficients must sum to 1, $\lambda_5 = 1 - \sum_{i=1}^{4} \lambda_i$, meaning that there 4 independent mixing coefficients. Each Gaussian component has two parameters, the mean and the variance, so there are $5 \times 2 = 10$ parameters in total. This gives a total of $4+10 = 14$ parameters.

For the ranges of these values, the mixing coefficients must be non-negative and sum to 1, so $0 \leq \lambda_i \leq 1$ and $\sum_{i=1}^{n} \lambda_i = 1$. The means $\mu_i$ can take any real value, and the variances $\sigma_i^2$ must be positive, so $\sigma_i^2 > 0$.

\subsection{}
\begin{mdframed}
    A function is concave if its second derivative is less than or equal to zero everywhere. Show that this is true for the function $g(x) = \log(x)$.
\end{mdframed}

The first derivative of $g(x) = \log(x)$ is:

\begin{equation*}
    \frac{d}{dx} \log(x) = \frac{1}{x}
\end{equation*}

The second derivative is:

\begin{equation*}
    \frac{d^{2}}{dx^{2}} \log(x) = -\frac{1}{x^{2}}
\end{equation*}

Since the second derivative is negative for all $x > 0$, the function $g(x) = \log(x)$ is concave for $x > 0$.
\newpage

\subsection{}
\begin{mdframed}
    For convex functions, Jensen's inequality works the other way around.

    \begin{equation*}
        g(\mathbb{E}[x]) \leq \mathbb{E}[g(x)]
        \tag{17.31}
    \end{equation*}

    A function is convex if its second derivative is greater than or equal to zero everywhere. Show that the function $g(x) = x^{2n}$ is convex for arbitrary $n \in [1,2, \dots]$. Use this result with Jensen's inequality to show that the square of the mean $\mathbb{E}[x]$ of a distribution $P(x)$ must be less than or equal to its second moment $\mathbb{E}[x^{2}]$.
\end{mdframed}

The first derivative of $g(x) = x^{2n}$ is:

\begin{equation*}
    \frac{d}{dx} x^{2n} = 2nx^{2n-1}
\end{equation*}

The second derivative is:

\begin{equation*}
    \frac{d^{2}}{dx^{2}} x^{2n} = 2n(2n-1)x^{2n-2}
\end{equation*}

Where $n \geq 1$, the second derivative is positive for all $x \in \mathbb{R}$ since the prefactor $2n(2n-1)$ is positive. Therefore, the function $g(x) = x^{2n}$ is convex for $n \in [1,2, \dots]$.

Applying Jensen's inequality to the convex function $g(x) = x^{2}$, we have:

\begin{equation*}
    \mathbb{E}[x^{2}] \geq \left( \mathbb{E}[x] \right)^{2}
\end{equation*}

This states that the mean of a distribution $P(x)$ squared is less than or equal to the second moment of the distribution.

\subsection{}
\begin{mdframed}
    Show that the ELBO, as expressed in equation 17.18, can alternatively be derived from the KL divergence between the variational distribution $q(\mathbf{z}| \mathbf{x})$ and the true posterior distribution $P(\mathbf{z}|\mathbf{x}, \boldsymbol{\phi})$:

    \begin{equation*}
        D_{KL}[q(\mathbf{z}|\mathbf{x})||P(\mathbf{z}|\mathbf{x}, \boldsymbol{\phi})] = \int q(\mathbf{z}|\mathbf{x}) \log \left[ \frac{q(\mathbf{z}|\mathbf{x})}{P(\mathbf{z}|\mathbf{x}, \boldsymbol{\phi})} \right] d\mathbf{z}
        \tag{17.32}
    \end{equation*}

    Start by using Bayes' rule (equation 17.19).
\end{mdframed}

\begin{align*}
    D_{KL}[q(\mathbf{z}|\mathbf{x})||P(\mathbf{z}|\mathbf{x}, \boldsymbol{\phi})] & = \int q(\mathbf{z}|\mathbf{x}) \log \left[ \frac{q(\mathbf{z}|\mathbf{x})}{P(\mathbf{z}|\mathbf{x}, \boldsymbol{\phi})} \right] d\mathbf{z} \\
\end{align*}


\subsection{}
\begin{mdframed}
    The reparameterization trick computes the derivative of an expression of a function $f(x)$:

    \begin{equation*}
        \frac{\partial}{\partial \boldsymbol{\phi}}\mathbb{E}_{P(x|\boldsymbol{\phi})}[f(x)]
    \end{equation*}

    with respect to the parameters $\boldsymbol{\phi}$ of the distribution $P(x|\boldsymbol{\phi})$. Show that this derivative can also be computed as:

    \begin{align*}
        \frac{\partial}{\partial \boldsymbol{\phi}}\mathbb{E}_{P(x|\boldsymbol{\phi})}[f(x)] & = \mathbb{E}_{P(x|\boldsymbol{\phi})} \left[ f(x) \frac{\partial}{\partial \boldsymbol{\phi}} \log(P(x|\boldsymbol{\phi})) \right] \\
                                                                                             & \approx \frac{1}{I} \sum_{i=1}^{I} f(x_{i})\frac{\partial}{\partial \boldsymbol{\phi}} \log(P(x|\boldsymbol{\phi})).
    \end{align*}

    This method is known as the REINFORCE algorithm or score function estimator.
\end{mdframed}

\subsection{}
\begin{mdframed}
    Why is it better to use spherical linear interpolation rather than regular linear interpolation when moving between points in the latent space? Hint: consider figure 8.13
\end{mdframed}

\subsection{}
\begin{mdframed}
    Derive the EM algorithm for the 1D mixture of Gaussians algorithm with $N$ components. To do this, you need to (i) find an expression for the posterior distribution $P(z|x)$ over the latent variable $z \in \{1, 2, \dots , N\}$ for a data point $x$ and (ii) find an expression that updates the evidence lower bound given the posterior distributions for all of the data points. You will need to use Lagrange multipliers to ensure that the weights $\lambda_{1}, \dots, \lambda_{N}$ of the Gaussians sum to one.
\end{mdframed}
\end{document}